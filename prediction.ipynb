{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77822c93",
   "metadata": {},
   "source": [
    "### 1.Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7971dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30c265",
   "metadata": {},
   "source": [
    "### 2. Preprocessing blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af46cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, use_ngrams=False, n_gram_range=(1, 1)):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses text: lowercasing, punctuation removal,\n",
    "    number removal, stop word removal, and tokenization.\n",
    "    Optionally generates n-grams.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "        \n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "\n",
    "    if use_ngrams:\n",
    "        all_ngrams = []\n",
    "        for n in range(n_gram_range[0], n_gram_range[1] + 1):\n",
    "            if n == 1:\n",
    "                all_ngrams.extend(filtered_tokens)\n",
    "            elif n > 1:\n",
    "                for i in range(len(filtered_tokens) - n + 1):\n",
    "                    all_ngrams.append(\"_\".join(filtered_tokens[i:i+n]))\n",
    "        return all_ngrams\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096814cc",
   "metadata": {},
   "source": [
    "### 3.Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff980fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_params(filename=\"naive_bayes_model_params.pkl\"):\n",
    "    \"\"\"Load the trained model parameters from a file.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f\"Model parameters loaded from {filename}\")\n",
    "    return (\n",
    "        data[\"vocabulary\"],\n",
    "        data[\"idf_scores\"],\n",
    "        data[\"class_priors\"],\n",
    "        data[\"word_probabilities\"],\n",
    "        data[\"total_words_in_class\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d272829",
   "metadata": {},
   "source": [
    "### 4.Predicting own blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f27d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_blog(doc_tokens, vocabulary, class_priors, word_probabilities, total_words_in_class, smoothing_alpha=1.0):\n",
    "    \"\"\"\n",
    "    Predicts the class label for a single document.\n",
    "    doc_tokens: list of preprocessed tokens for the document.\n",
    "    vocabulary: The global vocabulary (OrderedDict).\n",
    "    class_priors: Dictionary of class prior probabilities.\n",
    "    word_probabilities: Dictionary of word probabilities P(word | class).\n",
    "    total_words_in_class: Dictionary of total word counts per class (for smoothing).\n",
    "    smoothing_alpha: Laplace smoothing parameter.\n",
    "    Returns: The predicted class label.\n",
    "    \"\"\"\n",
    "    best_class = None\n",
    "    max_log_posterior = -float('inf')\n",
    "\n",
    "    for c, prior_prob in class_priors.items():\n",
    "        # Use log probabilities to avoid underflow\n",
    "        log_posterior = math.log(prior_prob)\n",
    "        \n",
    "        for word_token in doc_tokens:\n",
    "            # Only consider words present in the training vocabulary\n",
    "            if word_token in vocabulary:\n",
    "                # Get P(word | class) for this word in this class\n",
    "                word_prob = word_probabilities[c].get(word_token, smoothing_alpha / total_words_in_class[c])\n",
    "                log_posterior += math.log(word_prob)\n",
    "\n",
    "        if log_posterior > max_log_posterior:\n",
    "            max_log_posterior = log_posterior\n",
    "            best_class = c\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ee3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters loaded from blog_categorization.pkl\n",
      "Model parameters loaded successfully!\n",
      "\n",
      "New blog content: 'Far from the crowded trails of Everest and the busy alleys of Kathmandu lies a hidden gem in western Nepal—a place where the jungle speaks, the tiger prowls, and nature breathes freely. Welcome to Bardiya National Park (also spelled Bardia), Nepal’s largest and most undisturbed wilderness, where travelers come not just for sights, but for true encounters with the wild.'\n",
      "Processed tokens for new blog: ['far', 'crowded', 'trails', 'everest', 'busy', 'alleys', 'kathmandu', 'lies', 'hidden', 'gem']...\n",
      "\n",
      "Predicted category for the new blog: wildlife/nationalpark\n"
     ]
    }
   ],
   "source": [
    "#Define the file path for your saved model parameters\n",
    "model_filename = \"blog_categorization.pkl\"\n",
    "\n",
    "#Load the trained parameters\n",
    "try:\n",
    "    vocabulary_loaded, idf_scores_loaded, class_priors_loaded, word_probabilities_loaded, total_words_in_class_loaded = load_model_params(model_filename)\n",
    "    print(\"Model parameters loaded successfully!\")\n",
    "\n",
    "    # Define the new blog content you want to predict\n",
    "    new_blog_text = \"Far from the crowded trails of Everest and the busy alleys of Kathmandu lies a hidden gem in western Nepal—a place where the jungle speaks, the tiger prowls, and nature breathes freely. Welcome to Bardiya National Park (also spelled Bardia), Nepal’s largest and most undisturbed wilderness, where travelers come not just for sights, but for true encounters with the wild.\"\n",
    "    # You should use the same n-gram settings as during training\n",
    "    USE_NGRAMS_PREDICT = True # Set this to True if you trained with n-grams\n",
    "    NGRAM_RANGE_PREDICT = (1, 2) # Set this to the range used during training\n",
    "\n",
    "    print(f\"\\nNew blog content: '{new_blog_text}'\")\n",
    "\n",
    "    # Preprocess the new blog text using the same function and settings\n",
    "    processed_new_blog_tokens = preprocess_text(new_blog_text, use_ngrams=USE_NGRAMS_PREDICT, n_gram_range=NGRAM_RANGE_PREDICT)\n",
    "    print(f\"Processed tokens for new blog: {processed_new_blog_tokens[:10]}...\") # Show first few tokens\n",
    "\n",
    "    # Make a prediction using the loaded parameters\n",
    "    prediction_for_new_blog = predict_single_blog(\n",
    "        processed_new_blog_tokens,\n",
    "        vocabulary_loaded,\n",
    "        class_priors_loaded,\n",
    "        word_probabilities_loaded,\n",
    "        total_words_in_class_loaded\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPredicted category for the new blog: {prediction_for_new_blog}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model parameters file '{model_filename}' not found. Please ensure you have run the training pipeline and saved the model parameters.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or predicting: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
